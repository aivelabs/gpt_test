import streamlit as st
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import ChatMessage
from langchain_core.prompts import PromptTemplate, load_prompt
from langchain_openai import ChatOpenAI


# âœ… Web Search íˆ´ ì‚¬ìš© LLM ìƒì„±
def create_web_search_llm(model_name="gpt-4o"):
    return ChatOpenAI(
        model_name=model_name,
        tools=[{"type": "web_search_preview"}],
        tool_choice="auto",
    )


# âœ… í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì²´ì¸ ìƒì„±
def create_chain(prompt, model):
    return prompt | ChatOpenAI(model_name=model) | StrOutputParser()


# âœ… Streamlit UI ì„¤ì •
st.set_page_config(page_title="AIVE R&D Bot ğŸ’¬", page_icon="ğŸ’¬")
st.title("AVE R&D Bot ğŸ’¬")

# âœ… ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []


# âœ… ëŒ€í™” ì´ë ¥ ì¶œë ¥
def print_history():
    for msg in st.session_state["messages"]:
        st.chat_message(msg.role).write(msg.content)


# âœ… ëŒ€í™” ì´ë ¥ ì¶”ê°€
def add_history(role, content):
    st.session_state["messages"].append(ChatMessage(role=role, content=content))


# âœ… ì‚¬ì´ë“œë°” êµ¬ì„±
with st.sidebar:
    clear_btn = st.button("ëŒ€í™”ë‚´ìš© ì´ˆê¸°í™”")
    use_web_search = st.checkbox("ğŸ” ì›¹ ê²€ìƒ‰ ëª¨ë“œ í™œì„±í™”")
    tab1, tab2 = st.tabs(["í”„ë¡¬í”„íŠ¸", "í”„ë¦¬ì…‹"])

    default_prompt = (
        "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”."
    )
    user_text_prompt = tab1.text_area("í”„ë¡¬í”„íŠ¸", value=default_prompt)
    user_text_apply_btn = tab1.button("í”„ë¡¬í”„íŠ¸ ì ìš©", key="apply1")
    if user_text_apply_btn:
        tab1.markdown("âœ… í”„ë¡¬í”„íŠ¸ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤")
        prompt_template = user_text_prompt + "\n\n#Question:\n{question}\n\n#Answer:"
        prompt = PromptTemplate.from_template(prompt_template)
        st.session_state["chain"] = create_chain(prompt, "gpt-4o-mini")

    user_selected_prompt = tab2.selectbox("í”„ë¦¬ì…‹ ì„ íƒ", ["sns", "ë²ˆì—­", "ìš”ì•½"])
    user_selected_apply_btn = tab2.button("í”„ë¡¬í”„íŠ¸ ì ìš©", key="apply2")
    if user_selected_apply_btn:
        tab2.markdown("âœ… í”„ë¡¬í”„íŠ¸ê°€ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤")
        prompt = load_prompt(f"prompts/{user_selected_prompt}.yaml", encoding="utf8")
        st.session_state["chain"] = create_chain(prompt, "gpt-4o-mini")

# âœ… ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
if clear_btn:
    st.session_state["messages"].clear()

# âœ… ëŒ€í™” ì¶œë ¥
print_history()

# âœ… ì´ˆê¸° ì²´ì¸ ì„¤ì •
if "chain" not in st.session_state:
    prompt_template = user_text_prompt + "\n\n#Question:\n{question}\n\n#Answer:"
    prompt = PromptTemplate.from_template(prompt_template)
    st.session_state["chain"] = create_chain(prompt, "gpt-4o-mini")

# âœ… ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_input := st.chat_input():
    add_history("user", user_input)
    st.chat_message("user").write(user_input)

    with st.chat_message("assistant"):
        chat_container = st.empty()

        if use_web_search:
            # âœ… ì›¹ ê²€ìƒ‰ í¬í•¨ LLM ì‹¤í–‰
            llm = create_web_search_llm()
            response = llm.invoke(user_input)

            try:
                # âœ… ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ list í˜•íƒœì¼ ê²½ìš° ìì—°ìŠ¤ëŸ½ê²Œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                if isinstance(response.content, list):
                    texts = [
                        item["text"]
                        for item in response.content
                        if item["type"] == "text"
                    ]
                    ai_answer = "\n\n".join(texts)
                else:
                    ai_answer = response.content
            except Exception as e:
                ai_answer = f"âš ï¸ ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

            chat_container.markdown(ai_answer)
        else:
            # âœ… ê¸°ì¡´ ì²´ì¸ ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë°)
            stream_response = st.session_state["chain"].stream({"question": user_input})
            ai_answer = ""
            for chunk in stream_response:
                ai_answer += chunk
                chat_container.markdown(ai_answer)

        add_history("ai", ai_answer)
